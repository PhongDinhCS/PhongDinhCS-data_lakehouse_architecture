// Local_host
docker stop namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker rm namenode datanode datanode2 datanode3 resourcemanager nodemanager
docker-compose up -d namenode datanode datanode2 datanode3 resourcemanager nodemanager

// run this script in namenode after run docker-compose: 
// Local host
docker exec -it namenode bash
// inside namenode
hdfs dfs -chmod 777 /
-------------------------------------------------------------------------------

docker cp /home/ubuntu2020/phongdinhcs_project/phongdt_data_lakehouse_architecture_research/hadoop delta-spark:opt/spark/work-dir
delta-spark:/opt/spark/work-dir/hadoop/bin/hdfs
export PATH=/opt/spark/work-dir/hadoop/bin:$PATH
export HADOOP_USER_NAME=hdfs
hdfs dfs -mkdir hdfs://namenode:8020/delta-table

-------------------------------------------------------------------------------

// inside delta-spark
spark-shell --packages io.delta:delta-core_2.12:2.1.1
// scala
import org.apache.spark.sql.SparkSession
import io.delta.tables._

object DeltaOnHDFS {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Delta Lake on HDFS")
      .config("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.HDFSLogStore")
      .getOrCreate()

    // Create a DataFrame
    val data = Seq((1, "a"), (2, "b"), (3, "c")).toDF("id", "value")

    // Write DataFrame as a Delta Table
    data.write.format("delta").save("hdfs://namenode:8020/delta-table")

    spark.stop()
  }
}

DeltaOnHDFS.main(Array()) // Call the main method

---------------------------------------------------------------------------------
// git _ Local_host
git add .
git commit -m "take note this step"
git push origin main

